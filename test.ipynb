{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('./data/product_df.csv')\n",
    "df = df[['Star Rating', 'Comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star Rating    0\n",
      "Comment        0\n",
      "dtype: int64\n",
      "Các dòng dữ liệu trùng lặp trong cột 'Comment':\n",
      "      Star Rating                                   Comment\n",
      "48              4                                   Rất tốt\n",
      "76              5                                    Rất ok\n",
      "107             2                             pin tụt nhanh\n",
      "135             5                              sản phẩm tốt\n",
      "173             2                             hao pin nhanh\n",
      "177             5                              sản phẩm tốt\n",
      "181             5                                    Rất ok\n",
      "183             5                              sản phẩm tốt\n",
      "192             5                               sản phẩm ok\n",
      "205             5                               máy dùng ok\n",
      "225             4                                       Tốt\n",
      "227             5                               sản phẩm ok\n",
      "237             5                               máy dùng ok\n",
      "250             5                              sản phẩm tốt\n",
      "426             4                              máy dùng tốt\n",
      "435             5                                   Rất tốt\n",
      "461             4                                       Tốt\n",
      "516             5                                       Tốt\n",
      "526             4                                       Tốt\n",
      "529             4                                    Tạm ổn\n",
      "547             5                              Sản phẩm tốt\n",
      "549             5                                  Cũng tốt\n",
      "568             4                                  Hài lòng\n",
      "570             5                          Máy dùng rất tốt\n",
      "592             5                                       Tốt\n",
      "647             5                                   Rất tốt\n",
      "683             5                                       Tốt\n",
      "744             5                                       Tốt\n",
      "976             5                          Máy dùng rất tốt\n",
      "982             4                                 Tuyệt vời\n",
      "1153            5                                      Tốt.\n",
      "1160            5                          Máy sử dụng tốt.\n",
      "1163            5                                      Tốt.\n",
      "1237            5                                 Khá được.\n",
      "1244            4                                     Được.\n",
      "1260            5                             Rất hài lòng.\n",
      "1271            4                     Hiện tại sử dụng tốt.\n",
      "1272            5                         Tôi rất hài lòng.\n",
      "1273            4  Sản phẩm được. Hợp túi tiền dùng tạm ổn.\n",
      "1280            5                             Rất hài lòng.\n",
      "1283            5                                     Được.\n",
      "1311            5                             Dùng rất tốt.\n",
      "1328            5                  Điện thoại dùng rất tốt.\n",
      "1338            4                             Sản phẩm tốt.\n",
      "1409            5                                     Được.\n",
      "1443            4                                 Dùng tốt.\n",
      "Shape after dropping duplicates: (1498, 2)\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra và loại bỏ giá trị khuyết thiếu\n",
    "print(df.isnull().sum())\n",
    "df = df.dropna(subset=['Comment'])\n",
    "# Kiểm tra và loại bỏ dữ liệu trùng lặp\n",
    "duplicate_comments = df[df.duplicated(['Comment'])]\n",
    "print(\"Các dòng dữ liệu trùng lặp trong cột 'Comment':\")\n",
    "print(duplicate_comments)\n",
    "df = df.drop_duplicates(['Comment'])\n",
    "print(\"Shape after dropping duplicates:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Star Rating                                            Comment\n",
      "0            4                      điện thoại này dùng rất thích\n",
      "1            4                               sử dụng thấy cũng ok\n",
      "2            2                       bảo hành ít quá chỉ 12 tháng\n",
      "3            5                              sản phẩm mượt chạy êm\n",
      "4            3  cho mình hỏi muốn khởi động lại máy hay tắt ng...\n"
     ]
    }
   ],
   "source": [
    "def remove_special_characters(text):\n",
    "    # Loại bỏ các ký tự đặc biệt, giữ lại chữ cái, số, và các dấu câu\n",
    "    return re.sub(r'[^a-zA-ZÀ-ỹà-ỹ0-9\\s]', '', text)\n",
    "\n",
    "def to_lowercase(text):\n",
    "    # Chuyển đổi văn bản về chữ thường\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = remove_special_characters(text)\n",
    "    text = to_lowercase(text)\n",
    "    return text\n",
    "\n",
    "# Giả sử df là DataFrame của bạn và 'Comment' là cột chứa dữ liệu văn bản\n",
    "df['Comment'] = df['Comment'].apply(normalize_text)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Star Rating                                            Comment   Sentiment\n",
      "0            4                      điện thoại này dùng rất thích    tích cực\n",
      "1            4                               sử dụng thấy cũng ok    tích cực\n",
      "2            2                       bảo hành ít quá chỉ 12 tháng    tiêu cực\n",
      "3            5                              sản phẩm mượt chạy êm    tích cực\n",
      "4            3  cho mình hỏi muốn khởi động lại máy hay tắt ng...  trung tính\n"
     ]
    }
   ],
   "source": [
    "def label_sentiment(rating):\n",
    "    if rating in [1, 2]:\n",
    "        return 'tiêu cực'\n",
    "    elif rating == 3:\n",
    "        return 'trung tính'\n",
    "    elif rating in [4, 5]:\n",
    "        return 'tích cực'\n",
    "    else:\n",
    "        return 'không rõ'  # Nếu có xếp hạng nằm ngoài khoảng 1-5\n",
    "\n",
    "# Gắn nhãn cảm xúc cho mỗi đánh giá\n",
    "df['Sentiment'] = df['Star Rating'].apply(label_sentiment)\n",
    "\n",
    "# Hiển thị 5 hàng đầu tiên của dataframe với cột sentiment mới\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vuhamy\\anaconda3\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\vuhamy\\anaconda3\\Lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "  0%|          | 0/1587507 [00:00<?, ?it/s]Skipping token b'1587507' with 1-dimensional vector [b'100']; likely a header\n",
      "100%|██████████| 1587507/1587507 [03:57<00:00, 6674.38it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1587507, 100])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tải bộ word2vec đã được train sẵn dành cho tiếng việt\n",
    "import torch\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "word_embedding = vocab.Vectors(name = \"vi_word2vec.txt\",\n",
    "                               unk_init = torch.Tensor.normal_)\n",
    "\n",
    "word_embedding.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo hàm Get_Vector để truy xuất vector embedding từ bộ embedding đã tải\n",
    "def get_vector(embeddings, word):\n",
    "    \"\"\" Get embedding vector of the word\n",
    "    @param embeddings (torchtext.vocab.vectors.Vectors)\n",
    "    @param word (str)\n",
    "    @return vector (torch.Tensor)\n",
    "    \"\"\"\n",
    "    assert word in embeddings.stoi, f'*{word}* is not in the vocab!'\n",
    "    return embeddings.vectors[embeddings.stoi[word]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kiểm tra từ có trong từ điển hay không: Hàm sử dụng một lệnh assert để kiểm tra xem từ word có tồn tại trong từ điển của embeddings hay không. Nếu không tồn tại, nó sẽ hiển thị một thông báo lỗi.\n",
    "\n",
    "Trả về vector embedding: Nếu từ tồn tại trong từ điển, hàm sẽ sử dụng embeddings.stoi[word] để lấy chỉ số của từ trong danh sách các từ (từ điển), sau đó truy cập vào embeddings.vectors để lấy vector embedding tương ứng.\n",
    "\n",
    "Đầu ra là Vector embedding tương ứng với từ word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo hàm Get_Vector để tìm các từ gần nghĩa nhất với một từ cho trước, dựa trên mức độ tương tự trong không gian vector\n",
    "def closest_words(embeddings, vector, n=10):\n",
    "    \"\"\" Return n words closest in meaning to the word\n",
    "    @param embeddings (torchtext.vocab.vectors.Vectors)\n",
    "    @param vector (torch.Tensor)\n",
    "    @param n (int)\n",
    "    @return words (list(tuple(str, float)))\n",
    "    \"\"\"\n",
    "    distances = [(word, torch.dist(vector, get_vector(embeddings, word)).item())\n",
    "                 for word in embeddings.itos]\n",
    "\n",
    "    return sorted(distances, key = lambda w: w[1])[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tính toán khoảng cách: Hàm sử dụng một list comprehension để duyệt qua tất cả các từ trong từ điển (embeddings.itos). Với mỗi từ, nó sẽ tính khoảng cách Euclid giữa vector vector cho trước và vector embedding của từ đó sử dụng hàm torch.dist.\n",
    "\n",
    "Sắp xếp và trả về kết quả: Kết quả được sắp xếp theo khoảng cách tăng dần (key=lambda w: w[1]), và chỉ lấy n từ gần nghĩa nhất bằng cách sử dụng slicing [:n]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xây dựng từ điển"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\" The Vocabulary class is used to record words, which are used to convert\n",
    "        text to numbers and vice versa.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word2id = dict()\n",
    "        self.word2id['<pad>'] = 0   # Pad Token\n",
    "        self.word2id['<unk>'] = 1   # Unknown Token\n",
    "        self.unk_id = self.word2id['<unk>']\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        return self.word2id.get(word, self.unk_id)\n",
    "\n",
    "    def __contains__(self, word):\n",
    "        return word in self.word2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2id)\n",
    "\n",
    "    def id2word(self, word_index):\n",
    "        \"\"\"\n",
    "        @param word_index (int)\n",
    "        @return word (str)\n",
    "        \"\"\"\n",
    "        return self.id2word[word_index]\n",
    "\n",
    "    def add(self, word):\n",
    "        \"\"\" Add word to vocabulary\n",
    "        @param word (str)\n",
    "        @return index (str): index of the word just added\n",
    "        \"\"\"\n",
    "        if word not in self:\n",
    "            word_index = self.word2id[word] = len(self.word2id)\n",
    "            self.id2word[word_index] = word\n",
    "            return word_index\n",
    "        else:\n",
    "            return self[word]\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_corpus(corpus):\n",
    "        \"\"\"Split the documents of the corpus into words\n",
    "        @param corpus (list(str)): list of documents\n",
    "        @return tokenized_corpus (list(list(str))): list of words\n",
    "        \"\"\"\n",
    "        print(\"Tokenize the corpus...\")\n",
    "        tokenized_corpus = list()\n",
    "        for document in tqdm(corpus):\n",
    "            tokenized_document = [word.replace(\" \", \"_\") for word in word_tokenize(document)]\n",
    "            tokenized_corpus.append(tokenized_document)\n",
    "\n",
    "        return tokenized_corpus\n",
    "\n",
    "    def corpus_to_tensor(self, corpus, is_tokenized=False):\n",
    "        \"\"\" Convert corpus to a list of indices tensor\n",
    "        @param corpus (list(str) if is_tokenized==False else list(list(str)))\n",
    "        @param is_tokenized (bool)\n",
    "        @return indicies_corpus (list(tensor))\n",
    "        \"\"\"\n",
    "        if is_tokenized:\n",
    "            tokenized_corpus = corpus\n",
    "        else:\n",
    "            tokenized_corpus = self.tokenize_corpus(corpus)\n",
    "        indicies_corpus = list()\n",
    "        for document in tqdm(tokenized_corpus):\n",
    "            indicies_document = torch.tensor(list(map(lambda word: self[word], document)),\n",
    "                                             dtype=torch.int64)\n",
    "            indicies_corpus.append(indicies_document)\n",
    "\n",
    "        return indicies_corpus\n",
    "\n",
    "    def tensor_to_corpus(self, tensor):\n",
    "        \"\"\" Convert list of indices tensor to a list of tokenized documents\n",
    "        @param indicies_corpus (list(tensor))\n",
    "        @return corpus (list(list(str)))\n",
    "        \"\"\"\n",
    "        corpus = list()\n",
    "        for indicies in tqdm(tensor):\n",
    "            document = list(map(lambda index: self.id2word[index.item()], indicies))\n",
    "            corpus.append(document)\n",
    "\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Với bộ dữ liệu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGDD():\n",
    "    \"\"\" Load dataset from file csv\"\"\"\n",
    "\n",
    "    def __init__(self, vocab, csv_fpath='product_df.csv', tokenized_fpath=None):\n",
    "        \"\"\"\n",
    "        @param vocab (Vocabulary)\n",
    "        @param csv_fpath (str)\n",
    "        @param tokenized_fpath (str)\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.pad_idx = vocab[\"<pad>\"]\n",
    "        df = pd.read_csv(csv_fpath)\n",
    "        self.sentiments_list = list(df.sentiment)\n",
    "        self.reviews_list = list(df.vi_review)\n",
    "\n",
    "        sentiments_type = list(set(self.sentiments_list))\n",
    "        sentiments_type.sort()\n",
    "\n",
    "        self.sentiment2id = {sentiment: i for i, sentiment in enumerate(sentiments_type)}\n",
    "\n",
    "        if tokenized_fpath:\n",
    "            self.tokenized_reviews = torch.load(tokenized_fpath)\n",
    "        else:\n",
    "            self.tokenized_reviews = self.vocab.tokenize_corpus(self.reviews_list)\n",
    "\n",
    "        self.tensor_data = self.vocab.corpus_to_tensor(self.tokenized_reviews, is_tokenized=True)\n",
    "        self.tensor_label = torch.tensor([self.sentiment2id[sentiment] for sentiment in self.sentiments_list],\n",
    "                                         dtype=torch.float64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor_data[idx], self.tensor_label[idx]\n",
    "\n",
    "    def collate_fn(self, examples):\n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "\n",
    "        reviews = [e[0] for e in examples]\n",
    "        reviews = torch.nn.utils.rnn.pad_sequence(reviews,\n",
    "                                                  batch_first=False,\n",
    "                                                  padding_value=self.pad_idx)\n",
    "        reviews_lengths = torch.tensor([len(e[0]) for e in examples])\n",
    "        sentiments = torch.tensor([e[1] for e in examples])\n",
    "\n",
    "        return {\"Comment\": (reviews, reviews_lengths), \"Sentiment\": sentiments}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TGDD(vocab\u001b[38;5;241m=\u001b[39mvocab, csv_fpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_df.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenized_fpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m, in \u001b[0;36mTGDD.__init__\u001b[1;34m(self, vocab, csv_fpath, tokenized_fpath)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m@param vocab (Vocabulary)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m@param csv_fpath (str)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m@param tokenized_fpath (str)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m vocab\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_idx \u001b[38;5;241m=\u001b[39m vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_fpath)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentiments_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39msentiment)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "dataset = TGDD(vocab=vocab, csv_fpath=\"product_df.csv\", tokenized_fpath=\"tokenized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
