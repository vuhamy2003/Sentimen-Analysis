{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from underthesea import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nhập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải dữ liệu\n",
    "df = pd.read_csv('./data/product_df.csv')\n",
    "df = df[['Star Rating', 'Comment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gắn nhãn cho bộ dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Star Rating                                            Comment   Sentiment\n",
      "0            4                      Điện thoại này dùng rất thích    Tích cực\n",
      "1            4                               sử dụng thấy cũng ok    Tích cực\n",
      "2            2                      Bảo hành ít quá, chỉ 12 tháng    Tiêu cực\n",
      "3            5                             Sản phẩm mượt, chạy êm    Tích cực\n",
      "4            3  Cho mình hỏi muốn khởi động lại máy hay tắt ng...  Trung tính\n"
     ]
    }
   ],
   "source": [
    "def label_sentiment(rating):\n",
    "    if rating in [1, 2]:\n",
    "        return 'Tiêu cực'\n",
    "    elif rating == 3:\n",
    "        return 'Trung tính'\n",
    "    elif rating in [4, 5]:\n",
    "        return 'Tích cực'\n",
    "    else:\n",
    "        return 'Không xác định'  # Nếu có xếp hạng nằm ngoài khoảng 1-5\n",
    "\n",
    "# Gắn nhãn cảm xúc cho mỗi đánh giá\n",
    "df['Sentiment'] = df['Star Rating'].apply(label_sentiment)\n",
    "\n",
    "# Hiển thị 5 hàng đầu tiên của dataframe với cột sentiment mới\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star Rating    0\n",
      "Comment        0\n",
      "Sentiment      0\n",
      "dtype: int64\n",
      "Star Rating    0\n",
      "Comment        0\n",
      "Sentiment      0\n",
      "dtype: int64\n",
      "Các dòng dữ liệu trùng lặp trong cột 'Comment':\n",
      "      Star Rating                                   Comment Sentiment\n",
      "48              4                                   Rất tốt  Tích cực\n",
      "76              5                                    Rất ok  Tích cực\n",
      "107             2                             pin tụt nhanh  Tiêu cực\n",
      "135             5                              sản phẩm tốt  Tích cực\n",
      "173             2                             hao pin nhanh  Tiêu cực\n",
      "177             5                              sản phẩm tốt  Tích cực\n",
      "181             5                                    Rất ok  Tích cực\n",
      "183             5                              sản phẩm tốt  Tích cực\n",
      "192             5                               sản phẩm ok  Tích cực\n",
      "205             5                               máy dùng ok  Tích cực\n",
      "225             4                                       Tốt  Tích cực\n",
      "227             5                               sản phẩm ok  Tích cực\n",
      "237             5                               máy dùng ok  Tích cực\n",
      "250             5                              sản phẩm tốt  Tích cực\n",
      "426             4                              máy dùng tốt  Tích cực\n",
      "435             5                                   Rất tốt  Tích cực\n",
      "461             4                                       Tốt  Tích cực\n",
      "516             5                                       Tốt  Tích cực\n",
      "526             4                                       Tốt  Tích cực\n",
      "529             4                                    Tạm ổn  Tích cực\n",
      "547             5                              Sản phẩm tốt  Tích cực\n",
      "549             5                                  Cũng tốt  Tích cực\n",
      "568             4                                  Hài lòng  Tích cực\n",
      "570             5                          Máy dùng rất tốt  Tích cực\n",
      "592             5                                       Tốt  Tích cực\n",
      "647             5                                   Rất tốt  Tích cực\n",
      "683             5                                       Tốt  Tích cực\n",
      "744             5                                       Tốt  Tích cực\n",
      "976             5                          Máy dùng rất tốt  Tích cực\n",
      "982             4                                 Tuyệt vời  Tích cực\n",
      "1153            5                                      Tốt.  Tích cực\n",
      "1160            5                          Máy sử dụng tốt.  Tích cực\n",
      "1163            5                                      Tốt.  Tích cực\n",
      "1237            5                                 Khá được.  Tích cực\n",
      "1244            4                                     Được.  Tích cực\n",
      "1260            5                             Rất hài lòng.  Tích cực\n",
      "1271            4                     Hiện tại sử dụng tốt.  Tích cực\n",
      "1272            5                         Tôi rất hài lòng.  Tích cực\n",
      "1273            4  Sản phẩm được. Hợp túi tiền dùng tạm ổn.  Tích cực\n",
      "1280            5                             Rất hài lòng.  Tích cực\n",
      "1283            5                                     Được.  Tích cực\n",
      "1311            5                             Dùng rất tốt.  Tích cực\n",
      "1328            5                  Điện thoại dùng rất tốt.  Tích cực\n",
      "1338            4                             Sản phẩm tốt.  Tích cực\n",
      "1409            5                                     Được.  Tích cực\n",
      "1443            4                                 Dùng tốt.  Tích cực\n",
      "Shape after dropping duplicates: (1498, 3)\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra và loại bỏ giá trị khuyết thiếu\n",
    "print(df.isnull().sum())\n",
    "df = df.dropna(subset=['Comment'])\n",
    "# Kiểm tra và loại bỏ giá trị khuyết thiếu\n",
    "print(df.isnull().sum())\n",
    "df = df.dropna(subset=['Comment'])\n",
    "# Kiểm tra và loại bỏ dữ liệu trùng lặp\n",
    "duplicate_comments = df[df.duplicated(['Comment'])]\n",
    "print(\"Các dòng dữ liệu trùng lặp trong cột 'Comment':\")\n",
    "print(duplicate_comments)\n",
    "df = df.drop_duplicates(['Comment'])\n",
    "print(\"Shape after dropping duplicates:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Star Rating                                            Comment   Sentiment\n",
      "0            4                      điện thoại này dùng rất thích    Tích cực\n",
      "1            4                               sử dụng thấy cũng ok    Tích cực\n",
      "2            2                       bảo hành ít quá chỉ 12 tháng    Tiêu cực\n",
      "3            5                              sản phẩm mượt chạy êm    Tích cực\n",
      "4            3  cho mình hỏi muốn khởi động lại máy hay tắt ng...  Trung tính\n"
     ]
    }
   ],
   "source": [
    "# Chuẩn hóa và làm sạch văn bản\n",
    "def remove_special_characters(text):\n",
    "    # Loại bỏ các ký tự đặc biệt, giữ lại chữ cái, số, và các dấu câu\n",
    "    return re.sub(r'[^a-zA-ZÀ-ỹà-ỹ0-9\\s]', '', text)\n",
    "\n",
    "def to_lowercase(text):\n",
    "    # Chuyển đổi văn bản về chữ thường\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = remove_special_characters(text)\n",
    "    text = to_lowercase(text)\n",
    "    return text\n",
    "\n",
    "df['Comment'] = df['Comment'].apply(normalize_text)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu đánh giá vào cột X\n",
    "X = df['Comment'].values.tolist()\n",
    "# Lưu nhãn vào cột Y\n",
    "y = df['Sentiment'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tách từ, xây dựng bộ từ vựng và mã hóa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xây dựng hàm token từ underthesea\n",
    "def tokenize_and_build_vocab_vietnamese(comment):\n",
    "    tokens = word_tokenize(comment, format=\"text\")\n",
    "    return tokens.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=tokenize_and_build_vocab_vietnamese, token_pattern=None,\n",
    "                              max_features=5000, ngram_range=(1, 2), max_df=0.85, min_df=5)\n",
    "# Áp dụng với cột Comment\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "vocabulary = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mã hóa cho cột nhãn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cân bằng dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trước khi cân bằng: Counter({2: 832, 0: 428, 1: 238})\n",
      "Sau khi cân bằng: Counter({2: 832, 0: 832, 1: 832})\n"
     ]
    }
   ],
   "source": [
    "#Sử dụng SMOTE để cân bằng dữ liệu\n",
    "\n",
    "print('Trước khi cân bằng:', Counter(y))\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_tfidf, y)\n",
    "\n",
    "print('Sau khi cân bằng:', Counter(y_resampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước dữ liệu ban đầu: (1498, 1200)\n",
      "Kích thước dữ liệu sau khi cân bằng: (2496, 1200)\n",
      "Trước khi cân bằng: Counter({2: 832, 0: 428, 1: 238})\n",
      "Sau khi cân bằng: Counter({2: 832, 0: 832, 1: 832})\n"
     ]
    }
   ],
   "source": [
    "print('Kích thước dữ liệu ban đầu:', X_tfidf.shape)\n",
    "print('Kích thước dữ liệu sau khi cân bằng:', X_resampled.shape)\n",
    "# In ra số lượng mẫu của từng nhãn trước và sau khi cân bằng\n",
    "print('Trước khi cân bằng:', Counter(y))\n",
    "print('Sau khi cân bằng:', Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = X_tfidf.toarray()\n",
    "# Xóa dòng mã trên, và thay thế nó bằng dòng mã sau:\n",
    "X_lstm = X_tfidf.reshape(X_tfidf.shape[0], X_tfidf.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Reshape TF-IDF matrix for LSTM input\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_lstm \u001b[38;5;241m=\u001b[39m X_tfidf\u001b[38;5;241m.\u001b[39mtoarray()  \u001b[38;5;66;03m# Ensure X_tfidf is already TF-IDF transformed and converted to array\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Reshape to (number_of_samples, max_features, 1) for LSTM input\u001b[39;00m\n\u001b[0;32m      5\u001b[0m X_lstm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(X_lstm, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "# Reshape TF-IDF matrix for LSTM input\n",
    "X_lstm = X_tfidf.toarray()  # Ensure X_tfidf is already TF-IDF transformed and converted to array\n",
    "\n",
    "# Reshape to (number_of_samples, max_features, 1) for LSTM input\n",
    "X_lstm = np.expand_dims(X_lstm, axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chia tập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lstm, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xây dựng mô hình LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 1200, 1, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Embedding(input_dim\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(\u001b[38;5;241m128\u001b[39m))\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.2\u001b[39m))\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\vuhamy\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vuhamy\\anaconda3\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\vuhamy\\anaconda3\\Lib\\site-packages\\keras\\engine\\input_spec.py:235\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    233\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m shape\u001b[38;5;241m.\u001b[39mrank\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m--> 235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    237\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m         )\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 1200, 1, 128)"
     ]
    }
   ],
   "source": [
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=X_train.shape[1], output_dim=128, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "47/47 [==============================] - 3s 24ms/step - loss: 1.0373 - accuracy: 0.6119 - val_loss: 0.8980 - val_accuracy: 0.6987\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.6448 - accuracy: 0.8156 - val_loss: 0.4599 - val_accuracy: 0.8560\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.2888 - accuracy: 0.9158 - val_loss: 0.3198 - val_accuracy: 0.8853\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.1460 - accuracy: 0.9613 - val_loss: 0.3031 - val_accuracy: 0.8933\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0828 - accuracy: 0.9773 - val_loss: 0.2960 - val_accuracy: 0.9040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20675bf5350>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 3ms/step - loss: 0.2564 - accuracy: 0.9167\n",
      "Test Loss: 0.25642144680023193\n",
      "Test Accuracy: 0.9166666865348816\n"
     ]
    }
   ],
   "source": [
    "#Đánh giá mô hình\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tìm tham số tối ưu hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vuhamy\\AppData\\Local\\Temp\\ipykernel_13956\\2553851434.py:18: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8684034744898478 using {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.857047438621521 (0.03216138437832708) with: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 5, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8390113711357117 (0.026094892004427037) with: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 5, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8590514461199442 (0.029331314483772245) with: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 5, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8563794294993082 (0.02315955098328221) with: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 5, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.8643954594930013 (0.028917632927458248) with: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 10, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8610554536183676 (0.027976477175872365) with: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 10, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.862391451994578 (0.02629930573082417) with: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 10, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8630594412485758 (0.018023703624011924) with: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 10, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.8630594412485758 (0.026502120304641136) with: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8630594412485758 (0.019521204425677383) with: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8617234428723654 (0.02633320899152975) with: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8643954594930013 (0.019521241138999005) with: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.862391451994578 (0.023843121529925463) with: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 30, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8610554536183676 (0.018245149495051443) with: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 30, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.862391451994578 (0.025888874568108875) with: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 30, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8630594412485758 (0.01391628261607201) with: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 30, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.8523714145024618 (0.026299288067961623) with: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 5, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8370073437690735 (0.028167212888503938) with: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 5, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8657314578692118 (0.018439772395719298) with: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 5, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8496993978818258 (0.028948477667133928) with: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 5, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.8684034744898478 (0.02599209761531777) with: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8637274503707886 (0.021396944878322645) with: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8643954396247864 (0.023843110953771892) with: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8677354653676351 (0.022494949618899963) with: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 10, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.8663994669914246 (0.019521233660371756) with: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8650634487469991 (0.01802368742397624) with: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8643954594930013 (0.025419223819155557) with: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8650634487469991 (0.019383595020501635) with: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.8583834369977316 (0.018391318054585083) with: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 30, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8603874246279398 (0.020718851905517487) with: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 30, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8583834369977316 (0.02178958618193218) with: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 30, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8630594412485758 (0.018023703624011924) with: {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 30, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.84101535876592 (0.02650210478011776) with: {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 5, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8156312505404154 (0.03551110334632405) with: {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 5, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8496993978818258 (0.03260235414319843) with: {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 5, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8370073239008585 (0.022335679555334328) with: {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 5, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.865063468615214 (0.025047862540770332) with: {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 10, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8510354161262512 (0.025154539075043104) with: {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 10, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8603874246279398 (0.02477920704984036) with: {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 10, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8630594412485758 (0.022574148268812117) with: {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 10, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.8610554337501526 (0.025366521735184812) with: {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.859719435373942 (0.021271434791424716) with: {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.859719455242157 (0.02550685125030796) with: {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8650634487469991 (0.01952123230060633) with: {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.8583834568659464 (0.025888874568108875) with: {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 30, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8610554337501526 (0.023843100934258885) with: {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 30, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.859719435373942 (0.021645739596250275) with: {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 30, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8637274503707886 (0.018870343611431625) with: {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 30, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.8370073437690735 (0.027880598582588918) with: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 5, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8263193170229594 (0.023843121529925463) with: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 5, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8583834369977316 (0.02739626637115233) with: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 5, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8370073636372884 (0.032492683583513114) with: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 5, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.8610554337501526 (0.027880598582588918) with: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 10, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8583834369977316 (0.028917632927458248) with: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 10, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8650634487469991 (0.023674079362351313) with: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 10, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8663994669914246 (0.02744507203112536) with: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 10, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.8577154278755188 (0.02164575799064821) with: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8670674562454224 (0.021355203817368603) with: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8637274503707886 (0.028529246811963425) with: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.86773548523585 (0.01887033517150887) with: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.8603874444961548 (0.02609487369460656) with: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 30, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8630594412485758 (0.019521204425677386) with: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 30, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8610554536183676 (0.025681224128878178) with: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 30, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8617234428723654 (0.024434653930622458) with: {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 30, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.7508350014686584 (0.018822994158012297) with: {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 5, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.7849031289418539 (0.03456874464695431) with: {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 5, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8296593030293783 (0.02571594226595434) with: {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 5, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8283233046531677 (0.02744507203112536) with: {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 5, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.854375422000885 (0.017496045291507584) with: {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 10, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8256513079007467 (0.03260235414319843) with: {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 10, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8610554536183676 (0.025681224128878178) with: {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 10, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8470273812611898 (0.03499210575444126) with: {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 10, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.865063468615214 (0.021039424398531986) with: {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.859719435373942 (0.02319806395371628) with: {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.865063468615214 (0.02845097171423175) with: {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8630594611167908 (0.026602956998529253) with: {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.862391451994578 (0.02629930573082417) with: {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 30, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8657314578692118 (0.02723288399424128) with: {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 30, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8603874444961548 (0.029739261355292982) with: {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 30, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8623914321263632 (0.02178958618193218) with: {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 30, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.7795591155687968 (0.0410047063359772) with: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 5, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.7982631921768188 (0.040158015793286136) with: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 5, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8176352779070536 (0.02145943392326464) with: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 5, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8142952720324198 (0.0229271927734024) with: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 5, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.8476953903834025 (0.025559266059659445) with: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 10, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8376753528912863 (0.024434653930622458) with: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 10, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8563794294993082 (0.02315955098328221) with: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 10, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.8510353962580363 (0.03071359867983351) with: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 10, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.857047418753306 (0.022574148268812117) with: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8563794294993082 (0.021102965852266335) with: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.862391451994578 (0.02609490166794326) with: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.865063468615214 (0.025047862540770332) with: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'neurons': 128, 'optimizer': 'rmsprop'}\n",
      "0.859719435373942 (0.022907711471155598) with: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 30, 'neurons': 64, 'optimizer': 'adam'}\n",
      "0.8603874444961548 (0.025992069531340697) with: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 30, 'neurons': 64, 'optimizer': 'rmsprop'}\n",
      "0.8617234428723654 (0.027526230372822855) with: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 30, 'neurons': 128, 'optimizer': 'adam'}\n",
      "0.859719435373942 (0.02145943392326464) with: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 30, 'neurons': 128, 'optimizer': 'rmsprop'}\n"
     ]
    }
   ],
   "source": [
    "# Đặt seed cho tất cả các nguồn ngẫu nhiên để đảm bảo rằng mỗi lần chạy đều ra kết quả giống nhau\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)\n",
    "# Hàm xây dựng mô hình cho GridSearch\n",
    "def create_model(neurons, dropout_rate, optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(neurons // 2, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Xây dựng mô hình KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Tham số tìm kiếm GridSearch\n",
    "param_grid = {\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [5,10, 20, 30],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'neurons': [64, 128],\n",
    "    'dropout_rate': [0.2, 0.3]\n",
    "}\n",
    "\n",
    "# GridSearch\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Hiển thị kết quả\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(f\"{mean} ({std}) with: {param}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "47/47 [==============================] - 7s 25ms/step - loss: 1.0580 - accuracy: 0.4603 - val_loss: 0.9691 - val_accuracy: 0.7067\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.8105 - accuracy: 0.7869 - val_loss: 0.6391 - val_accuracy: 0.8613\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4631 - accuracy: 0.8898 - val_loss: 0.4121 - val_accuracy: 0.8827\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.2680 - accuracy: 0.9305 - val_loss: 0.3277 - val_accuracy: 0.8907\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.1658 - accuracy: 0.9619 - val_loss: 0.3009 - val_accuracy: 0.9147\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.1097 - accuracy: 0.9806 - val_loss: 0.3030 - val_accuracy: 0.9093\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0752 - accuracy: 0.9846 - val_loss: 0.3032 - val_accuracy: 0.9067\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.0498 - accuracy: 0.9947 - val_loss: 0.3082 - val_accuracy: 0.9040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2067b9fbe90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lấy tham số tốt nhất\n",
    "best_params = grid_result.best_params_\n",
    "\n",
    "# Tạo mô hình mới với tham số tốt nhất\n",
    "final_model = create_model(neurons=best_params['neurons'],\n",
    "                           optimizer=best_params['optimizer'],\n",
    "                           dropout_rate=best_params['dropout_rate'])\n",
    "\n",
    "# Huấn luyện mô hình mới\n",
    "final_model.fit(X_train, y_train,epochs=best_params['epochs'], batch_size=best_params['batch_size'], \n",
    "                          validation_data=(X_val, y_val), \n",
    "                          callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kiểm thử và dự đoán mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2552 - accuracy: 0.9199\n",
      "Test Loss: 0.25520509481430054\n",
      "Test Accuracy: 0.9198718070983887\n"
     ]
    }
   ],
   "source": [
    "# Đánh giá mô hình với tham số mới\n",
    "score = final_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 408ms/step\n",
      "Đánh giá: Sản phẩm tốt, rất hài lòng!\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "\n",
      "Đánh giá: Pin yếu, cam tệ, màn hình giật lag liên tục\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "\n",
      "Đánh giá: Chỉ sử dụng ở mức tạm ổn\n",
      "Dự đoán cảm xúc: Trung tính\n",
      "\n",
      "20/20 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Tiêu cực       0.92      0.92      0.92       209\n",
      "  Trung tính       0.87      0.96      0.91       204\n",
      "    Tích cực       0.97      0.88      0.92       211\n",
      "\n",
      "    accuracy                           0.92       624\n",
      "   macro avg       0.92      0.92      0.92       624\n",
      "weighted avg       0.92      0.92      0.92       624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Đánh giá mới cần kiểm thử\n",
    "new_reviews = [\n",
    "    \"Sản phẩm tốt, rất hài lòng!\",\n",
    "    \"Pin yếu, cam tệ, màn hình giật lag liên tục\",\n",
    "    \"Chỉ sử dụng ở mức tạm ổn\"\n",
    "]\n",
    "\n",
    "# Tiền xử lý đánh giá mới\n",
    "def preprocess_new_reviews(reviews):\n",
    "    preprocessed_reviews = [normalize_text(review) for review in reviews]\n",
    "    return preprocessed_reviews\n",
    "\n",
    "preprocessed_reviews = preprocess_new_reviews(new_reviews)\n",
    "\n",
    "# Chuyển đổi đánh giá mới sang TF-IDF\n",
    "new_reviews_tfidf = vectorizer.transform(preprocessed_reviews).toarray()\n",
    "\n",
    "# Dự đoán nhãn cho đánh giá mới\n",
    "predictions = final_model.predict(new_reviews_tfidf)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Chuyển đổi nhãn dự đoán từ số sang tên\n",
    "predicted_sentiments = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# In kết quả\n",
    "for review, sentiment in zip(new_reviews, predicted_sentiments):\n",
    "    print(f\"Đánh giá: {review}\")\n",
    "    print(f\"Dự đoán cảm xúc: {sentiment}\")\n",
    "    print()\n",
    "\n",
    "# Báo cáo đánh giá mô hình trên tập test\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
