{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./data/product_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1544, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= df[['Star Rating', 'Comment']]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star Rating    0\n",
      "Comment        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Kiểm tra dữ liệu khuyết thiếu\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các dòng dữ liệu trùng lặp trong cột 'Comment':\n",
      "      Star Rating                                   Comment\n",
      "48              4                                   Rất tốt\n",
      "76              5                                    Rất ok\n",
      "107             2                             pin tụt nhanh\n",
      "135             5                              sản phẩm tốt\n",
      "173             2                             hao pin nhanh\n",
      "177             5                              sản phẩm tốt\n",
      "181             5                                    Rất ok\n",
      "183             5                              sản phẩm tốt\n",
      "192             5                               sản phẩm ok\n",
      "205             5                               máy dùng ok\n",
      "225             4                                       Tốt\n",
      "227             5                               sản phẩm ok\n",
      "237             5                               máy dùng ok\n",
      "250             5                              sản phẩm tốt\n",
      "426             4                              máy dùng tốt\n",
      "435             5                                   Rất tốt\n",
      "461             4                                       Tốt\n",
      "516             5                                       Tốt\n",
      "526             4                                       Tốt\n",
      "529             4                                    Tạm ổn\n",
      "547             5                              Sản phẩm tốt\n",
      "549             5                                  Cũng tốt\n",
      "568             4                                  Hài lòng\n",
      "570             5                          Máy dùng rất tốt\n",
      "592             5                                       Tốt\n",
      "647             5                                   Rất tốt\n",
      "683             5                                       Tốt\n",
      "744             5                                       Tốt\n",
      "976             5                          Máy dùng rất tốt\n",
      "982             4                                 Tuyệt vời\n",
      "1153            5                                      Tốt.\n",
      "1160            5                          Máy sử dụng tốt.\n",
      "1163            5                                      Tốt.\n",
      "1237            5                                 Khá được.\n",
      "1244            4                                     Được.\n",
      "1260            5                             Rất hài lòng.\n",
      "1271            4                     Hiện tại sử dụng tốt.\n",
      "1272            5                         Tôi rất hài lòng.\n",
      "1273            4  Sản phẩm được. Hợp túi tiền dùng tạm ổn.\n",
      "1280            5                             Rất hài lòng.\n",
      "1283            5                                     Được.\n",
      "1311            5                             Dùng rất tốt.\n",
      "1328            5                  Điện thoại dùng rất tốt.\n",
      "1338            4                             Sản phẩm tốt.\n",
      "1409            5                                     Được.\n",
      "1443            4                                 Dùng tốt.\n"
     ]
    }
   ],
   "source": [
    "#Kiểm tra dữ liệu trùng lặp\n",
    "duplicate_comments = df[df.duplicated(['Comment'])]\n",
    "print(\"Các dòng dữ liệu trùng lặp trong cột 'Comment':\")\n",
    "print(duplicate_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1498, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates(['Comment'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Làm sạch dữ liệu văn bản"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    # Loại bỏ các ký tự đặc biệt, giữ lại chữ cái, số, và các dấu câu\n",
    "    return re.sub(r'[^a-zA-ZÀ-ỹà-ỹ0-9\\s.,!?]', '', text)\n",
    "\n",
    "def to_lowercase(text):\n",
    "    # Chuyển đổi văn bản về chữ thường\n",
    "    return text.lower()\n",
    "\n",
    "def add_space_around_punctuation(text):\n",
    "    # Tạo khoảng cách giữa từ và dấu câu\n",
    "    text = re.sub(r'([.,!?])', r' \\1 ', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)  # Loại bỏ khoảng trắng thừa\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = remove_special_characters(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = add_space_around_punctuation(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Star Rating                                            Comment\n",
      "0               4                      điện thoại này dùng rất thích\n",
      "1               4                               sử dụng thấy cũng ok\n",
      "2               2                     bảo hành ít quá , chỉ 12 tháng\n",
      "3               5                            sản phẩm mượt , chạy êm\n",
      "4               3  cho mình hỏi muốn khởi động lại máy hay tắt ng...\n",
      "...           ...                                                ...\n",
      "1539            1  đem máy ra cho nhân viên xem mà về máy vẫn nón...\n",
      "1540            3                       cũng được dù pin xuống nhanh\n",
      "1541            1  hơi hối hận khi mua máy vội mà chưa tìm hiểu k...\n",
      "1542            4  chụp ảnh ổn , điện tử ổn , màn hình đẹp , pin ...\n",
      "1543            5  mua mới đã là ios 17 rồi nhé . nhân viên tư vấ...\n",
      "\n",
      "[1498 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Áp dụng hàm clean_text và thay thế cột 'comment'\n",
    "df['Comment'] = df['Comment'].apply(clean_text)\n",
    "\n",
    "# In DataFrame sau khi làm sạch\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loại bỏ stopwords\n",
    "# Đọc dữ liệu từ file vietnamese-stopwords.txt\n",
    "with open('vietnamese-stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = [word.strip() for word in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm để loại bỏ stopwords\n",
    "def remove_stopwords(comment):\n",
    "    words = comment.split()\n",
    "    cleaned_words = [word for word in words if word.lower() not in stopwords]\n",
    "    return ' '.join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Star Rating                                            Comment\n",
      "0               4                               điện thoại rất thích\n",
      "1               4                                         sử dụng ok\n",
      "2               2                                      bảo hành , 12\n",
      "3               5                            sản phẩm mượt , chạy êm\n",
      "4               3                         khởi động máy tắt thao tác\n",
      "...           ...                                                ...\n",
      "1539            1                       đem máy nhân viên máy nóng ,\n",
      "1540            3                                                pin\n",
      "1541            1              hơi hối hận mua máy vội kĩ , máy chán\n",
      "1542            4  chụp ảnh ổn , điện tử ổn , màn hình đẹp , pin ...\n",
      "1543            5           mua ios 17 . nhân viên tư vấn nhiệt tình\n",
      "\n",
      "[1498 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Áp dụng hàm và thay thế cột 'comment'\n",
    "df['Comment'] = df['Comment'].apply(remove_stopwords)\n",
    "\n",
    "# In DataFrame sau khi loại bỏ\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tách từ và mã hóa từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munderthesea\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Tạo một hàm để tách từ cho mỗi comment trong cột 'Comment'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vuhamy\\anaconda3\\Lib\\site-packages\\underthesea\\__init__.py:38\u001b[0m\n\u001b[0;32m     33\u001b[0m     __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m ex\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# TOP-LEVEL MODULES\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msent_tokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_normalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text_normalize\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword_tokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "File \u001b[1;32mc:\\Users\\vuhamy\\anaconda3\\Lib\\site-packages\\underthesea\\pipeline\\sent_tokenize\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m join, dirname\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PunktSentenceTokenizer\n\u001b[0;32m      8\u001b[0m sentence_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_model\u001b[39m():\n",
      "File \u001b[1;32mc:\\Users\\vuhamy\\anaconda3\\Lib\\site-packages\\nltk\\__init__.py:180\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download, download_shell\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vuhamy\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:2479\u001b[0m\n\u001b[0;32m   2469\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   2472\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[0;32m   2473\u001b[0m \u001b[38;5;66;03m# Main:\u001b[39;00m\n\u001b[0;32m   2474\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2477\u001b[0m \n\u001b[0;32m   2478\u001b[0m \u001b[38;5;66;03m# Aliases\u001b[39;00m\n\u001b[1;32m-> 2479\u001b[0m _downloader \u001b[38;5;241m=\u001b[39m Downloader()\n\u001b[0;32m   2480\u001b[0m download \u001b[38;5;241m=\u001b[39m _downloader\u001b[38;5;241m.\u001b[39mdownload\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_shell\u001b[39m():\n",
      "File \u001b[1;32mc:\\Users\\vuhamy\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:515\u001b[0m, in \u001b[0;36mDownloader.__init__\u001b[1;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m# decide where we're going to save things to.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_download_dir()\n",
      "File \u001b[1;32mc:\\Users\\vuhamy\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:1071\u001b[0m, in \u001b[0;36mDownloader.default_download_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1067\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;66;03m# Check if we have sufficient permissions to install in a\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;66;03m# variety of system-wide locations.\u001b[39;00m\n\u001b[1;32m-> 1071\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nltkdir \u001b[38;5;129;01min\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mpath:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(nltkdir) \u001b[38;5;129;01mand\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39minternals\u001b[38;5;241m.\u001b[39mis_writable(nltkdir):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m nltkdir\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Khởi tạo các thành phần nltk nếu cần thiết (ngoài underthesea)\n",
    "nltk.download('punkt')  # Đảm bảo các tài nguyên NLTK cần thiết được tải xuống\n",
    "\n",
    "# Tạo một hàm để tách từ cho mỗi comment trong cột 'Comment'\n",
    "def tokenize_comment(comment):\n",
    "    tokens = word_tokenize(comment)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Áp dụng hàm và thay thế cột 'comment'\n",
    "df['Comment'] = df['Comment'].apply(tokenize_comment)\n",
    "\n",
    "# In DataFrame sau khi loại bỏ\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
